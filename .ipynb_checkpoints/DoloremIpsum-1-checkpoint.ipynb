{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3cdb359-9981-4873-b597-4bad4e6e71ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Dolorem Ipsum:\n",
    "## Steganography Through Random Fake Latin Generation\n",
    "\n",
    "#### Inspired in part by _Perfectly Secure Steganography Using Minimum Entropy Coupling_: https://arxiv.org/abs/2210.14889\n",
    "\n",
    "## Overview:\n",
    "\n",
    "* Use mildly illegitimate behavior to mask serious infractions:  \n",
    "  \"I'm sorry, I've been using down time at work to do some web design on the side, and sending out sample layouts to my clients via email\" \"...oh by the way the lorem ipsum placeholder text in my layouts is really stego text with a hidden message -- or exfiltrated files!\"\n",
    "* Generate random Latin-ish text, different every time, using a valid known method and source corpus, but with a specified secret text embedded.  In this demo, the random output words are chosen such that **second** letter of each word is a vowel if the corresponding hidden bit is 0, and a consonant if it is 1.\n",
    "  * In this demo the secret message is ascii and plaintext, but any binary could be encoded, including compressed or encrypted data.\n",
    "  * The bandwidth of this implementation is quite low -- one bit of secret data per 64 bits or so of cover/output content -- but with a larger source corpus more refinements could be made\n",
    "\n",
    "### Special features:\n",
    "\n",
    "* The source data structure is precomputed, with the encoding trickery baked in, so that nothing sneaky need be done on the user's immediate machine.\n",
    "* The encoding script is fairly straightforward, and makes no mention of stego or exfil, and works perfectly well as a legitimate lorem ipsum generator\n",
    "* The input secret text is passed implicitly by placing a file with a given name and the desired contents in the same directory as the script.\n",
    "  * The script opens a file of the same name, in append mode, and downloads the markov chain data structure file, placing it after the secret message in that file, and closes the write.\n",
    "  * The script then *opens* the same file, extracts the message and the data structures into memory, then deletes the file.  On examination after the fact, no suspicious files will be present -- and if the script is run a second time, even looking at deleted files will only show a pure copy of the downloaded data structures.\n",
    "\n",
    "## Technology\n",
    "\n",
    "### Markov Chain Monte Carlo\n",
    "\n",
    "In order to generate Latin-like text with verisimilitude, statistics are gathered from a valid Latin corpus, then used to generate arbitrary text with similar statistics.\n",
    "\n",
    "* The largest single Latin corpus obtained was the Vulgate Bible, compiled in the year 382, with over 600,000 words.\n",
    "https://www.biblestudyguide.org/ebooks/bibles/vul.pdf\n",
    "  * All English introductions, copyright info, and chapter titles, as well as all numerals -- mostly this was chapter, verse, and page numbers -- and punctuation, were removed\n",
    "  * Several proper nouns were removed which were ubiquitous and would have heavily flavored the output as Biblically sourced\n",
    "  * The remaining proper nouns, as identified by initial capital letters, were simply modified by removing that initial letter.\n",
    "  * Some shorter words, 1-2 letters, were removed, because their ubuiquity might have highlighted the encoding method more than was desirable.\n",
    "  * Everything remaining was treated as a single long chain of words, ignoring breaks in line, verse, sentence, paragraph, page, chapter, and book.\n",
    "\n",
    "#### Markov Chain\n",
    "\n",
    "This series of words was digested into a data structure whereby for each consecutive set of three words, a Markov Chain (MC), the first two words were used as dictionary indexes, and the third was added to a set of words found at the indexed location.  This window was slid across the entire text, with each word taking all three roles successively, and wrapped at the end so the text was processed as a loop instead of a line.\n",
    "\n",
    "To the above standard Markov Chain analysis was added the further modification that, at each indexed location in the data structure, **two** sets were retained, the first with words having a vowel at their second position, and the second with words having a consonant at the same place.\n",
    "Also, a separate pair of sets was built, one with all the second-letter-vowel words ad one with all the second-letter-consonant words, and stored separately from the Markov Chain tree.\n",
    "\n",
    "#### Monte Carlo\n",
    "\n",
    "To generate text from a Markov Chain tree, the Markov Chain Monte Carlo (MCMC!) method is used:\n",
    "* Select any two words found consecutively in the original corpus (trivially, use the first two words)  \n",
    "\n",
    "* (*) Using those two words as indexes, look in the MC tree for the set of possible following words, and pick one at random.  Add that word to the output buffer\n",
    "\n",
    "* Update the words in hand by selecting the second word and the new one to replace the former first and second.\n",
    "\n",
    "* Repeat from (*) until the output buffer has the desired length\n",
    "\n",
    "    #### Modified Characteristic Markov Chain Monte Carlo, Encoding = MCÂ³\n",
    "\n",
    "* In the presented algorithm (MCMCMC!!), after indexing into the data structure, two sets will be found instead of one.\n",
    "\n",
    "* Using each consecutive bit of the hidden message in turn, select the first set on 0 and the second set on 1.\n",
    "\n",
    "* Often, because of the corpus' innate statistics, it will not be the case that both 0 and 1 sets have candidate words. In this case, select a \"next index\" word from the nonempty set, but select a \"next output\" word from the provided separate word sets outside the MC tree.\n",
    "  * This will affect the statistics of the output text, and could provide a weakness for steganalysis to exploit.\n",
    "  * This weakness SHOULD be mitigated because of all the corpus processing that occurred before statistical collection, meaning the output text will diverge far from both the Vulgate Bible and Latin in general, in terms of text statistics.\n",
    "  * It is possible this could be avoided by generating several permutations of the vulgate text, and ingesting all of them to build the MC tree, so that the \"cache misses\" are arbitrarily rare.\n",
    "\n",
    "* Just for fun, further modifications were made to split the output into random-length sentences and comma-separated clauses.  This does not affect the stego properties of the text.\n",
    "\n",
    "### Data-Hiding Script and Process\n",
    "* Definitions/Assumptions:\n",
    "  * The desired action is data exfiltration\n",
    "  * The data to be stolen exists on computers at a location designated \"work\"\n",
    "  * The actor has access to other computers at a location designated \"home\"\n",
    "  * The Dolorem Ipsum tool is hosted on a server with full source code and usage instructions, designated \"dark\"\n",
    "  * The second script (encoding, designated \"loremipsum\") and json-encoded MC data structure file are hosted on another server which appears to be a legitimate web-developer's tool, designated \"dev\"\n",
    "* Procedure:\n",
    "  1. The actor does their research on the \"dark\" site from home, and does not bring any code with them to work\n",
    "  1. At work, the actor accesses the \"dev\" site, with whatever pretext behaviors like reading simplified instructions or comparing tools they deem appropriate.  Eventually, they download the \"loremipsum.py\" script to a local file store\n",
    "  1. The actor then prepares a file, in the sample called \"markov.txt\", which contains their desired exfil content, in any format. (as currently written, the left-curly-brace character should be avoided)\n",
    "  1. The actor executes the loremipsum.py script, which accesses the \"words.json\" db file from the \"dev\" site, and downloads it to the local file system, **APPENDING** it to the \"markov.txt\" file\n",
    "  1. The script then reads that same file, splitting it on the first-seen left-curly-brace character.  Everything to the left is treated as the secret message, and everything to the right is MC database\n",
    "  1. The MC database is then decoded and used in the MCMCMC algorithm, using consecutive bits of the secret message to select the word sets at each state, as described in the \"technology\" section above\n",
    "  1. The script then _deletes_ the markov.txt file, ostensible to save hard drive space, deleting the source message text along with it.\n",
    "  1. The actor can then use the generated _lorem ipsum_ text however they like.  One suggested method would be to create mock web site layouts, using the stego text as placeholders, then send those layouts to an external email, asking a purported client for input on the design.\n",
    "  1. The generated lorem ipsum is saved as lorem.txt and can be sent off site directly or as part of an osentible web site layout design\n",
    "  1. As an added precaution, the actor could run the script again, with an empty starting file, so hard drive analysis would show a clean copy of the db file\n",
    "  1. Once home, the actor can obtain the third script from the \"dark\" site, and use it to extract the original message/files from the emailed placeholder text.\n",
    "  1. Any later forensics investigation would uncover the \"loremipsum.py\" script and references to the \"dev\" site, which would look innocuous and function exactly as advertised.\n",
    "  1. Profit!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12628d01-9860-4aae-ba0f-5a693fb20a8c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Script 1:\n",
    "Using the Vulgate corpus, generate the Markov Chain data structure and save it to a file.\n",
    "\n",
    "This file should be hosted at an innocuous location, and the second script edited to refer to its url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c2d01918-8555-40d9-9181-bd3396bde6f1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "import random;\n",
    "import json;\n",
    "\n",
    "n = 2;\n",
    "usewds = []\n",
    "start = []\n",
    "recent = []\n",
    "rem=''\n",
    "mark = {}\n",
    "wordlist = [set(),set()]\n",
    "\n",
    "with open('./vulgate.txt', mode=\"rb\") as f:\n",
    "    while True:\n",
    "        k = f.read(10)\n",
    "        if len(k) == 0: \n",
    "            if len(start) > 0:\n",
    "                usewds = start[:]\n",
    "                start.clear()\n",
    "            else:\n",
    "                break;\n",
    "        s = rem + str(k, encoding='utf-8')\n",
    "        i=s.rindex(' ')\n",
    "        div = s[:i]\n",
    "        rem = s[i:]\n",
    "        wds = usewds or div.split(\" \")\n",
    "        for w in wds:\n",
    "            if w=='': continue;\n",
    "            if len(start) < n and len(k) > 0: start.append(w)\n",
    "            if len(recent) == n:\n",
    "                a = recent[-2]\n",
    "                b = recent[-1]\n",
    "                if not (a in mark): \n",
    "                    mark[a] = {}\n",
    "                if not (b in mark[a]): \n",
    "                    fresh = [set(), set()]\n",
    "                    mark[a][b] = fresh\n",
    "                try:\n",
    "                    z = 0 if w[1] in ['a', 'e', 'i', 'o', 'u'] else 1\n",
    "                    mark[a][b][z].add(w)\n",
    "                    wordlist[z].add(w)\n",
    "                except: print(w)\n",
    "            recent.append(w)\n",
    "            if len(recent) > n: recent.pop(0)\n",
    "wordlist[0] = list(wordlist[0])\n",
    "wordlist[1] = list(wordlist[1])\n",
    "\n",
    "mark = {a: {c: [list(e) for e in d] for c, d in b.items()} for a, b in mark.items()}\n",
    "\n",
    "out = open('./words.json', 'w')\n",
    "json.dump({'mark':mark,'wordlist':wordlist}, out)\n",
    "out.close()\n",
    "\n",
    "print(\"ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea0530e-30b0-4729-a88b-b148f5fbbab4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Prep Steps:\n",
    "1. User downloads the below script, script 2, from a site that makes no reference to these instructions or scripts 1 or 3.\n",
    "1. User creates a file, 'markov.txt', in the same directory as the script, with contents consisting of the message or data to be hidden.\n",
    "1. User executes script, which will delete 'markov.txt' and generate 'lorem.txt' with the lorem ipsum to be sent out directly or used as placeholder text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9b35b926-d513-4aff-bb8a-d4bb04f803fc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'markov.txt'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in practice this step would be manual\n",
    "import shutil\n",
    "shutil.copyfile('message.txt', 'markov.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ef49f8-a626-46d6-914e-3b5aaf06c557",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Script 2:\n",
    "\n",
    "Using the source markov.txt and url to the words.json database:\n",
    "1. download the database\n",
    "2. append the db contents to the markov.txt file\n",
    "3. read the markov.txt file to load message text and db content into memory\n",
    "4. delete markov.txt file \"to save space\"\n",
    "5. generate lorem ipsum text with the encoded message embedded\n",
    "6. save lorem ipsum to \"lorem.txt\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8b0c2ea3-da5f-45bd-8186-4694967f575d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Download source markov chain db\n",
    "import urllib.request\n",
    "u = 'https://ibiwan.com/static/words.json'\n",
    "f = open ('./markov.txt', 'a');\n",
    "for line in urllib.request.urlopen(u):\n",
    "    f.write(line.decode('utf-8'))\n",
    "f.close()\n",
    "\n",
    "# \n",
    "m, v = open('markov.txt', 'r', encoding='utf-8').read().split(\"{\",1);\n",
    "k = json.loads('{'+v)\n",
    "mark = k['mark']\n",
    "wordlist = k['wordlist']\n",
    "\n",
    "import os\n",
    "# recover word database space\n",
    "os.remove('markov.txt')\n",
    "\n",
    "\n",
    "w1 = random.choice(list(mark.keys()))\n",
    "w2 = random.choice(list(mark[w1].keys()))\n",
    "bites = [bin(ord(b))[2:].rjust(8, '0') for b in m]\n",
    "bs = ''.join(bites)\n",
    "gen = [w1, w2]\n",
    "out = []\n",
    "sentenceLength = 0\n",
    "minSentenceLength = 5\n",
    "minClauseLength = 6\n",
    "for c in bs:\n",
    "    branch = int(c)\n",
    "    [a, b] = gen[-2:]\n",
    "    cands = []\n",
    "    outWord = ''\n",
    "    vecWord = ''\n",
    "    if a in mark and b in mark[a]:\n",
    "        cands = list(mark[a][b][branch])\n",
    "        if len(cands):\n",
    "            vecWord = random.choice(cands)\n",
    "            outWord = vecWord\n",
    "        else:\n",
    "            vecWord = random.choice(list(mark[a][b][1 - branch]))\n",
    "            outWord = random.choice(wordlist[branch])\n",
    "    elif a in mark:\n",
    "        print(a, b, outWord, vecWord, mark[a][branch].keys(),mark[a][1 - branch].keys(), 'GEN', gen)\n",
    "        print(mark[a])\n",
    "        break\n",
    "    else:\n",
    "        outWord = random.choice(cands) if len(cands) > 0 else random.choice(wordlist[1-branch])\n",
    "        vecWord = random.choice(list(mark[a][b][1-branch]))\n",
    "\n",
    "    endSen = False\n",
    "    endClause = False\n",
    "    if sentenceLength > minSentenceLength:\n",
    "        for j in range(sentenceLength - minSentenceLength):\n",
    "            br = random.randint(0,5) == 0\n",
    "            if br:\n",
    "                endSen = True\n",
    "                break;\n",
    "    if sentenceLength == 0:\n",
    "        outWord = outWord.capitalize()\n",
    "    if sentenceLength > minClauseLength:\n",
    "        for k in range(sentenceLength - minClauseLength):\n",
    "            br = random.randint(0, 1) == 0\n",
    "            if br:\n",
    "                endClause = True\n",
    "                break;\n",
    "    if endSen:\n",
    "        out.append(outWord + '.')\n",
    "        sentenceLength = 0\n",
    "    elif endClause:\n",
    "        out.append(outWord + ',')\n",
    "        sentenceLength = 1\n",
    "    else:\n",
    "        out.append(outWord)\n",
    "        sentenceLength += 1\n",
    "    gen = [b, vecWord]\n",
    "lorem = \" \".join(out) + '.'\n",
    "\n",
    "out = open('./lorem.txt', 'w')\n",
    "out.write(lorem)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e37712c2-7f97-4f93-b573-266895234074",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do it tomorrow night!\n",
      "Signed, Me.\n"
     ]
    }
   ],
   "source": [
    "lorem = open('./lorem.txt','r').read()\n",
    "bits = [\"0\" if word[1] in ['a', 'e', 'i', 'o', 'u'] else \"1\" for word in lorem.split(\" \")]\n",
    "message = \"\"\n",
    "while len(bits) > 0:\n",
    "    bitbyte = bits[0:8]\n",
    "    bites = ''.join(bitbyte)\n",
    "    val = int(bites, 2)\n",
    "    message += chr(val)\n",
    "    bits = bits[8:]\n",
    "print(message)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
